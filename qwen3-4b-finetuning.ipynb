{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install --no-deps unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:05:02.476667Z","iopub.execute_input":"2025-06-28T09:05:02.476892Z","iopub.status.idle":"2025-06-28T09:05:17.195635Z","shell.execute_reply.started":"2025-06-28T09:05:02.476874Z","shell.execute_reply":"2025-06-28T09:05:17.194347Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**Chargement du mod√®le**","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-4B\",\n    max_seq_length = 1048,  \n    load_in_4bit = True,     \n    load_in_8bit = False,    \n    full_finetuning = False,\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:05:23.027258Z","iopub.execute_input":"2025-06-28T09:05:23.027548Z","iopub.status.idle":"2025-06-28T09:06:27.090588Z","shell.execute_reply.started":"2025-06-28T09:05:23.027519Z","shell.execute_reply":"2025-06-28T09:06:27.089637Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-28 09:05:41.375768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751101541.624810      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751101541.673484      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.6.8: Fast Qwen3 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65e66cfdd4f94a7d8102231981722836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2afb78e58cdc4acea3bb11b81e075238"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d716c0042d4d25855ad5de4dfa4b58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6876a9519f4c029fd1304b6ddcd5c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9ff943d5b6b4a6c8361c13e562587f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed7fde720ce45f1b284ed7e7d1a139b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd75b7fb7fc249bca7f21f47c8160856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf95291827084c3883f6e5e105bb2a36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d64e55703fc4e7c89603d7ecf6fe089"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"**Configuration de LoRA avec un rang r√©duit (r=4), sans dropout, ciblant les couches de projection pour un affinage efficace et peu co√ªteux.**","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 4,          \n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 8, \n    lora_dropout = 0,\n    bias = \"none\",    \n    \n    use_gradient_checkpointing = \"unsloth\", \n    random_state = 3407,\n    use_rslora = False,   \n    loftq_config = None,  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:06:34.592792Z","iopub.execute_input":"2025-06-28T09:06:34.594044Z","iopub.status.idle":"2025-06-28T09:06:41.412464Z","shell.execute_reply.started":"2025-06-28T09:06:34.593998Z","shell.execute_reply":"2025-06-28T09:06:41.411460Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.6.8 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Chargement de la dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"AbdelilahFdg/QA\", split = \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:06:47.149485Z","iopub.execute_input":"2025-06-28T09:06:47.149833Z","iopub.status.idle":"2025-06-28T09:06:48.998848Z","shell.execute_reply.started":"2025-06-28T09:06:47.149805Z","shell.execute_reply":"2025-06-28T09:06:48.998205Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"QA-darija.parquet:   0%|          | 0.00/14.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05acd3148acc434b9fafa25ef1532df5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/65878 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b82a36d2f82478290670eeb85c545f7"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"**Division du jeu de donn√©es en ensembles d‚Äôentra√Ænement (80‚ÄØ%), validation (10‚ÄØ%) et test (10‚ÄØ%), avec √©chantillonnage de 30 exemples pour l‚Äô√©valuation rapide.**","metadata":{}},{"cell_type":"code","source":"train_ratio = 0.8\neval_ratio = 0.1 \n\n# M√©lange al√©atoire du dataset\ndataset = dataset.shuffle(seed=42)\ndataset = dataset.select(range(10000))\n\n# Calcul des tailles\ntotal = len(dataset)\ntrain_size = int(total * train_ratio)\neval_size = int(total * eval_ratio)\ntest_size = total - train_size - eval_size\n\n# Division\ntrain_dataset = dataset.select(range(train_size))\neval_dataset = dataset.select(range(train_size, train_size + eval_size))\ntest_dataset = dataset.select(range(train_size + eval_size, total))\neval_dataset = eval_dataset.select(range(30))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:06:52.519815Z","iopub.execute_input":"2025-06-28T09:06:52.520598Z","iopub.status.idle":"2025-06-28T09:06:52.569922Z","shell.execute_reply.started":"2025-06-28T09:06:52.520539Z","shell.execute_reply":"2025-06-28T09:06:52.569089Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"eval_dataset.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:06:57.190771Z","iopub.execute_input":"2025-06-28T09:06:57.191088Z","iopub.status.idle":"2025-06-28T09:06:57.197415Z","shell.execute_reply.started":"2025-06-28T09:06:57.191063Z","shell.execute_reply":"2025-06-28T09:06:57.196481Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(30, 1)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"**Application du template de conversation du tokenizer pour formater les dialogues en vue de l'entra√Ænement, de la validation et du test.**","metadata":{}},{"cell_type":"code","source":"\n\ntrain_dataset = tokenizer.apply_chat_template(\n    train_dataset[\"conversations\"],\n    tokenize = False,\n)\neval_dataset = tokenizer.apply_chat_template(\n    eval_dataset[\"conversations\"],\n    tokenize = False,\n)\ntest_dataset = tokenizer.apply_chat_template(\n    test_dataset[\"conversations\"],\n    tokenize = False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:07:00.380882Z","iopub.execute_input":"2025-06-28T09:07:00.381791Z","iopub.status.idle":"2025-06-28T09:07:01.482389Z","shell.execute_reply.started":"2025-06-28T09:07:00.381763Z","shell.execute_reply":"2025-06-28T09:07:01.481409Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:07:06.368635Z","iopub.execute_input":"2025-06-28T09:07:06.368959Z","iopub.status.idle":"2025-06-28T09:07:06.373915Z","shell.execute_reply.started":"2025-06-28T09:07:06.368933Z","shell.execute_reply":"2025-06-28T09:07:06.373023Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>user\nÿ¥ŸÜŸà ŸáŸà ÿßŸÑÿ™ŸÅÿ≥Ÿäÿ± ÿßŸÑŸÖÿ≠ÿ™ŸÖŸÑ ŸÑŸÑŸÅÿ±ŸÇ ÿßŸÑŸÉÿ®Ÿäÿ± ÿ®ŸäŸÜ ŸÖÿπÿØŸÑ ÿßŸÑÿπŸÖÿ± ÿπŸÜÿØ ÿßŸÑÿ≤Ÿàÿßÿ¨ ÿßŸÑÿ£ŸàŸÑ ÿ®ŸäŸÜ ÿßŸÑÿ±ÿ¨ÿßŸÑ ŸàÿßŸÑŸÜÿ≥ÿßÿ°ÿü<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nÿÆÿ∑ÿ£ ŸÅ ÿ™ÿ≥ÿ¨ŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™<|im_end|>\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"**Ajout d‚Äôun message syst√®me en darija en t√™te de chaque exemple pour guider le style des r√©ponses g√©n√©r√©es.**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nsystem_msg = \"<|im_start|>system\\nÿ¨ÿßŸàÿ® ÿØŸäŸÖÿß ŸÅÿßŸÑÿØŸëÿßÿ±ÿ¨ÿ© ÿßŸÑŸÖÿ∫ÿ±ÿ®Ÿäÿ©ÿå ÿ®ÿßŸÑÿ≠ÿ±ŸàŸÅ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©.\\n<|im_end|>\\n\"\n\ntrain_dataset = pd.Series(train_dataset, name=\"text\")\neval_dataset = pd.Series(eval_dataset, name=\"text\")\ntest_dataset = pd.Series(test_dataset, name=\"text\")\n\n# Ajouter le message syst√®me au d√©but de chaque exemple\ntrain_dataset = train_dataset.apply(\n    lambda x: system_msg + x if system_msg.strip() not in x else x\n)\n\neval_dataset = eval_dataset.apply(\n    lambda x: system_msg + x if system_msg.strip() not in x else x\n)\n\ntest_dataset = test_dataset.apply(\n    lambda x: system_msg + x if system_msg.strip() not in x else x\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:07:11.937472Z","iopub.execute_input":"2025-06-28T09:07:11.938309Z","iopub.status.idle":"2025-06-28T09:07:11.960306Z","shell.execute_reply.started":"2025-06-28T09:07:11.938279Z","shell.execute_reply":"2025-06-28T09:07:11.959498Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Conversion des ensembles d'entra√Ænement, validation et test en objets Dataset compatibles avec la biblioth√®que Transformers.**","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(pd.DataFrame(train_dataset))\neval_dataset = Dataset.from_pandas(pd.DataFrame(eval_dataset))\ntest_dataset = Dataset.from_pandas(pd.DataFrame(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:07:16.559899Z","iopub.execute_input":"2025-06-28T09:07:16.560222Z","iopub.status.idle":"2025-06-28T09:07:16.632870Z","shell.execute_reply.started":"2025-06-28T09:07:16.560197Z","shell.execute_reply":"2025-06-28T09:07:16.631979Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**Installation des biblioth√®ques n√©cessaires pour le calcul des m√©triques d‚Äô√©valuation (BERTScore, ROUGE, BLEU, etc.).**","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install evaluate\n!pip install bert_score\n!pip install rouge_score\n!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:07:31.276867Z","iopub.execute_input":"2025-06-28T09:07:31.277404Z","iopub.status.idle":"2025-06-28T09:09:00.831688Z","shell.execute_reply.started":"2025-06-28T09:07:31.277375Z","shell.execute_reply":"2025-06-28T09:09:00.830433Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"**Configuration de la gestion m√©moire CUDA pour permettre une allocation dynamique via des segments extensibles.**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:09:09.796142Z","iopub.execute_input":"2025-06-28T09:09:09.796524Z","iopub.status.idle":"2025-06-28T09:09:09.801450Z","shell.execute_reply.started":"2025-06-28T09:09:09.796491Z","shell.execute_reply":"2025-06-28T09:09:09.800631Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**Fonction de calcul des m√©triques d'√©valuation (BERTScore, BLEU, ROUGE, CHRF)**","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nimport torch\n\n# Chargement des m√©triques\nbertscore = evaluate.load(\"bertscore\")\nbleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")\nchrf = evaluate.load(\"chrf\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # Convert logits to token IDs if necessary\n    if isinstance(preds, np.ndarray) and len(preds.shape) == 3:\n        preds = np.argmax(preds, axis=-1)\n\n    # Replace -100 in labels with pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n    # Convert to lists\n    preds = preds.tolist() if isinstance(preds, np.ndarray) else preds\n    labels = labels.tolist() if isinstance(labels, np.ndarray) else labels\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds = [p.strip() for p in decoded_preds]\n    decoded_labels = [l.strip() for l in decoded_labels]\n\n    # BERTScore (sur CPU pour √©viter OOM)\n    result_bertscore = bertscore.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        lang=\"ar\",  # langue \"ar\" m√™me si c'est darija\n        device=\"cpu\"\n    )\n\n    # BLEU\n    try:\n        result_bleu = bleu.compute(\n            predictions=decoded_preds,\n            references=[[l] for l in decoded_labels]\n        )\n        bleu_score = result_bleu[\"bleu\"]\n    except Exception as e:\n        print(f\"BLEU computation failed: {e}\")\n        bleu_score = 0.0\n\n    # ROUGE\n    result_rouge = rouge.compute(\n        predictions=decoded_preds,\n        references=decoded_labels\n    )\n\n    # CHRF\n    result_chrf = chrf.compute(\n        predictions=decoded_preds,\n        references=decoded_labels\n    )\n\n    # Compilation des m√©triques\n    metrics = {\n        \"eval_bertscore_f1\": np.mean(result_bertscore[\"f1\"]),\n        \"eval_bleu\": bleu_score,\n        \"eval_rougeL\": result_rouge[\"rougeL\"],\n        \"eval_chrf\": result_chrf[\"score\"],\n    }\n\n    # Log console\n    print(\n        f\"√âvaluation - BERTScore F1: {metrics['eval_bertscore_f1']:.4f}, \"\n        f\"BLEU: {metrics['eval_bleu']:.4f}, \"\n        f\"ROUGE-L: {metrics['eval_rougeL']:.4f}, \"\n        f\"CHRF: {metrics['eval_chrf']:.4f}\",\n        flush=True\n    )\n\n    return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:09:17.940659Z","iopub.execute_input":"2025-06-28T09:09:17.940998Z","iopub.status.idle":"2025-06-28T09:09:20.882270Z","shell.execute_reply.started":"2025-06-28T09:09:17.940973Z","shell.execute_reply":"2025-06-28T09:09:20.881503Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c693631125c42eaba935503d97c357d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e4931576404e73ae8d4ca585949a79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee172199c08845e1baeb57d2ad57d209"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"629f0523df224924839eace102241f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bff1858cf49946019d9ca3e179586ead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7303a15242b64666b5f8c427f13974fa"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"**Configuration et initialisation du SFTTrainer pour fine-tuning supervis√©**","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    compute_metrics = compute_metrics,\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 2,\n        per_device_eval_batch_size = 1,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 60,\n        # max_steps = 30,\n        num_train_epochs = 4,\n        learning_rate = 2e-5,\n        logging_steps = 200,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\",\n        output_dir = \"/kaggle/working/checkpoints\",\n        save_steps = 200,\n        eval_strategy = \"steps\",  # <-- active l'√©valuation automatique\n        eval_steps = 200,                \n        gradient_checkpointing = True,\n        max_seq_length=650,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        log_level=\"info\",  # Ensure info-level logs are shown\n        logging_strategy=\"steps\",\n        disable_tqdm=False,\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:09:49.403611Z","iopub.execute_input":"2025-06-28T09:09:49.404902Z","iopub.status.idle":"2025-06-28T09:09:56.258917Z","shell.execute_reply.started":"2025-06-28T09:09:49.404873Z","shell.execute_reply":"2025-06-28T09:09:56.257836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b76f45223d4928907b29fa3a4e550b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"978eab7ffb17420196b78920ba1147d9"}},"metadata":{}},{"name":"stderr","text":"Using auto half precision backend\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"**Modifie le trainer pour entra√Æner uniquement sur les r√©ponses de l'assistant**","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|im_start|>user\\n\",\n    response_part = \"<|im_start|>assistant\\n\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:10:05.326299Z","iopub.execute_input":"2025-06-28T09:10:05.326778Z","iopub.status.idle":"2025-06-28T09:10:07.300034Z","shell.execute_reply.started":"2025-06-28T09:10:05.326748Z","shell.execute_reply":"2025-06-28T09:10:07.299112Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1bacb250b504bb0a86ad920cb6ffc67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"900eee5574df432cb0b556e3f25429ab"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"**Entrainement du modele**","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:10:13.894184Z","iopub.execute_input":"2025-06-28T09:10:13.894531Z","iopub.status.idle":"2025-06-28T13:55:09.758714Z","shell.execute_reply.started":"2025-06-28T09:10:13.894499Z","shell.execute_reply":"2025-06-28T13:55:09.758065Z"}},"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 8,000 | Num Epochs = 4 | Total steps = 2,000\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 8,257,536/4,000,000,000 (0.21% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 4:44:38, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bertscore F1</th>\n      <th>Bleu</th>\n      <th>Rougel</th>\n      <th>Chrf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>2.830700</td>\n      <td>2.246203</td>\n      <td>0.585097</td>\n      <td>0.027571</td>\n      <td>0.455502</td>\n      <td>32.477842</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.185300</td>\n      <td>2.083238</td>\n      <td>0.590567</td>\n      <td>0.029183</td>\n      <td>0.485718</td>\n      <td>33.489250</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.026100</td>\n      <td>1.979778</td>\n      <td>0.588638</td>\n      <td>0.029200</td>\n      <td>0.474186</td>\n      <td>33.733307</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.965500</td>\n      <td>1.898172</td>\n      <td>0.590768</td>\n      <td>0.030157</td>\n      <td>0.479987</td>\n      <td>33.991980</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.883100</td>\n      <td>1.840976</td>\n      <td>0.590545</td>\n      <td>0.030580</td>\n      <td>0.486891</td>\n      <td>34.372767</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.847300</td>\n      <td>1.800295</td>\n      <td>0.589709</td>\n      <td>0.030646</td>\n      <td>0.439000</td>\n      <td>34.467795</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.802800</td>\n      <td>1.773094</td>\n      <td>0.588872</td>\n      <td>0.030780</td>\n      <td>0.438445</td>\n      <td>34.648487</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.781300</td>\n      <td>1.749910</td>\n      <td>0.590373</td>\n      <td>0.031144</td>\n      <td>0.441646</td>\n      <td>34.661663</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.779200</td>\n      <td>1.743914</td>\n      <td>0.590207</td>\n      <td>0.031111</td>\n      <td>0.483415</td>\n      <td>34.803794</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.763400</td>\n      <td>1.736543</td>\n      <td>0.591879</td>\n      <td>0.031470</td>\n      <td>0.491040</td>\n      <td>34.728028</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\nUnsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"281522ca6de944b6932ff37d6c430d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c22ba31d7c14e2cb1ec507198767b15"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfa27d9ceccb45069459e942794d16e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38f1d48c39043c39796904d909af781"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.51.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f401402462374d7db7eae94080500a39"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\nA pretrained model of type `BertModel` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n* `bert.encoder.layer.9.output.LayerNorm.beta` -> `encoder.layer.9.output.LayerNorm.bias`\n* `bert.encoder.layer.9.output.LayerNorm.gamma` -> `encoder.layer.9.output.LayerNorm.weight`\nIf you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5851, BLEU: 0.0276, ROUGE-L: 0.4555, CHRF: 32.4778\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5906, BLEU: 0.0292, ROUGE-L: 0.4857, CHRF: 33.4892\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5886, BLEU: 0.0292, ROUGE-L: 0.4742, CHRF: 33.7333\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-600\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5908, BLEU: 0.0302, ROUGE-L: 0.4800, CHRF: 33.9920\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-800\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5905, BLEU: 0.0306, ROUGE-L: 0.4869, CHRF: 34.3728\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-1000\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5897, BLEU: 0.0306, ROUGE-L: 0.4390, CHRF: 34.4678\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-1200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5889, BLEU: 0.0308, ROUGE-L: 0.4384, CHRF: 34.6485\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-1400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5904, BLEU: 0.0311, ROUGE-L: 0.4416, CHRF: 34.6617\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-1600\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5902, BLEU: 0.0311, ROUGE-L: 0.4834, CHRF: 34.8038\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-1800\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 30\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"√âvaluation - BERTScore F1: 0.5919, BLEU: 0.0315, ROUGE-L: 0.4910, CHRF: 34.7280\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoints/checkpoint-2000\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--qwen3-4b-unsloth-bnb-4bit/snapshots/bbe12ebf9362b3bcbf44e826e4c98910f2c02e72/config.json\nModel config Qwen3Config {\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\",\n      \"model.layers.4.mlp\",\n      \"model.layers.6.self_attn\",\n      \"model.layers.34.self_attn\",\n      \"model.layers.33.self_attn\",\n      \"model.layers.4.self_attn\",\n      \"model.layers.34.mlp\",\n      \"model.layers.1.self_attn\",\n      \"model.layers.1.mlp\",\n      \"model.layers.0.mlp\",\n      \"model.layers.0.self_attn\",\n      \"model.layers.3.mlp\",\n      \"model.layers.6.mlp\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(trainer_stats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T11:02:08.600899Z","iopub.execute_input":"2025-06-29T11:02:08.601655Z","iopub.status.idle":"2025-06-29T11:02:08.609560Z","shell.execute_reply.started":"2025-06-29T11:02:08.601631Z","shell.execute_reply":"2025-06-29T11:02:08.608447Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4267228944.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'trainer_stats' is not defined"],"ename":"NameError","evalue":"name 'trainer_stats' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"**√âvaluation du mod√®le sur le jeu de test en utilisant les m√©triques d√©finies**","metadata":{}},{"cell_type":"code","source":"results = trainer.evaluate(\n    eval_dataset=test_dataset,\n    metric_key_prefix=\"test\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Pousse le mod√®le quantifi√© au format GGUF vers le HF Hub avec le tokenizer associ√©.**","metadata":{}},{"cell_type":"code","source":" # Pushing to HF Hub\n model.push_to_hub_gguf(\"AbdelilahFdg/darija-chatt\", tokenizer, quantization_method = \"q4_k_m\", token = \"x\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T18:38:55.104257Z","iopub.execute_input":"2025-06-05T18:38:55.104853Z","iopub.status.idle":"2025-06-05T18:46:22.086618Z","shell.execute_reply.started":"2025-06-05T18:38:55.104828Z","shell.execute_reply":"2025-06-05T18:46:22.085877Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nSubmodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'ggml/src/ggml-kompute/kompute'\nCloning into '/kaggle/working/llama.cpp/ggml/src/ggml-kompute/kompute'...\nSubmodule path 'ggml/src/ggml-kompute/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\nRequirement already satisfied: gguf in /usr/local/lib/python3.11/dist-packages (0.17.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from gguf) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from gguf) (6.0.2)\nRequirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.11/dist-packages (from gguf) (0.2.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from gguf) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->gguf) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->gguf) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->gguf) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->gguf) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->gguf) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->gguf) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->gguf) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->gguf) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->gguf) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->gguf) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->gguf) (2024.2.0)\nmake: Entering directory '/kaggle/working/llama.cpp'\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n-- Configuring done (1.7s)\n-- Generating done (0.3s)\n-- Build files have been written to: /kaggle/working/llama.cpp/build\n[  0%] Generating build details from Git\n[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\n[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n[ 10%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n[ 10%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n[ 10%] Built target build_info\n[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n[ 13%] Linking CXX static library libggml-base.a\n[ 13%] Built target ggml-base\n[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n[ 17%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n[ 20%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\n[ 20%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\n[ 20%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\n[ 20%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\n[ 24%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n[ 24%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n[ 27%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\n[ 27%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\n[ 31%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\n[ 31%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\n[ 31%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n[ 34%] Linking CXX static library libggml-cpu.a\n[ 34%] Built target ggml-cpu\n[ 37%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n[ 37%] Linking CXX static library libggml.a\n[ 37%] Built target ggml\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n[ 41%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n[ 41%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o\n[ 58%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-recurrent.cpp.o\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\n[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n[ 72%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n[ 72%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n[ 72%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n[ 75%] Linking CXX static library libllama.a\n[ 75%] Built target llama\n[ 79%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n[ 79%] Building CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\n[ 82%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o\n[ 82%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n[ 82%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n[ 86%] Building CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\n[ 86%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n[ 89%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\n[ 89%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n[ 93%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n[ 93%] Building CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\n[ 93%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n[ 96%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n[ 96%] Linking CXX static library libcommon.a\n[ 96%] Built target common\n[100%] Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n[100%] Linking CXX executable ../../bin/llama-quantize\n[100%] Built target llama-quantize\n[  0%] Built target build_info\n[ 14%] Built target ggml-base\n[ 35%] Built target ggml-cpu\n[ 39%] Built target ggml\n[ 78%] Built target llama\n[100%] Built target common\n[100%] Building CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\n[100%] Linking CXX executable ../../bin/llama-export-lora\n[100%] Built target llama-export-lora\n[  0%] Built target build_info\n[ 13%] Built target ggml-base\n[ 34%] Built target ggml-cpu\n[ 37%] Built target ggml\n[ 75%] Built target llama\n[ 96%] Built target common\n[100%] Building CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\n[100%] Linking CXX executable ../../bin/llama-cli\n[100%] Built target llama-cli\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 3.6G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 20.86 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:01<00:00, 29.53it/s]\nModel config Qwen3Config {\n  \"_attn_implementation_autoset\": true,\n  \"architectures\": [\n    \"Qwen3ForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 151645,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2560,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 9728,\n  \"max_position_embeddings\": 40960,\n  \"max_window_layers\": 36,\n  \"model_type\": \"qwen3\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 151654,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.51.3\",\n  \"unsloth_fixed\": true,\n  \"unsloth_version\": \"2025.5.9\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\n","output_type":"stream"},{"name":"stderr","text":"Configuration saved in AbdelilahFdg/darija-chatt/config.json\nConfiguration saved in AbdelilahFdg/darija-chatt/generation_config.json\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving AbdelilahFdg/darija-chatt/pytorch_model-00001-of-00002.bin...\nUnsloth: Saving AbdelilahFdg/darija-chatt/pytorch_model-00002-of-00002.bin...\n","output_type":"stream"},{"name":"stderr","text":"The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at AbdelilahFdg/darija-chatt/pytorch_model.bin.index.json.\n","output_type":"stream"},{"name":"stdout","text":"Done.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Converting qwen3 model. Can use fast conversion = False.\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n \"-____-\"     In total, you will have to wait at least 16 minutes.\n\nUnsloth: Installing llama.cpp. This might take 3 minutes...\nUnsloth: [1] Converting model at AbdelilahFdg/darija-chatt into f16 GGUF format.\nThe output location will be /kaggle/working/AbdelilahFdg/darija-chatt/unsloth.F16.gguf\nThis might take 3 minutes...\nWriting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.05G/8.05G [00:22<00:00, 354Mbyte/s] \nUnsloth: Conversion completed! Output location: /kaggle/working/AbdelilahFdg/darija-chatt/unsloth.F16.gguf\nUnsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\nmain: build = 5599 (1caae7fc)\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: quantizing '/kaggle/working/AbdelilahFdg/darija-chatt/unsloth.F16.gguf' to '/kaggle/working/AbdelilahFdg/darija-chatt/unsloth.Q4_K_M.gguf' as Q4_K_M using 8 threads\nllama_model_loader: loaded meta data with 25 key-value pairs and 398 tensors from /kaggle/working/AbdelilahFdg/darija-chatt/unsloth.F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Darija Chatt\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   5:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   6:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv   7:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv   8:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv   9:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  10:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  12:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  13:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  14:                          general.file_type u32              = 1\nllama_model_loader: - kv  15:               general.quantization_version u32              = 2\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type  f16:  253 tensors\n[   1/ 398]                   output_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[   2/ 398]                    token_embd.weight - [ 2560, 151936,     1,     1], type =    f16, converting to q6_K .. size =   741.88 MiB ->   304.28 MiB\n[   3/ 398]                  blk.0.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[   4/ 398]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[   5/ 398]               blk.0.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[   6/ 398]             blk.0.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[   7/ 398]                  blk.0.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[   8/ 398]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[   9/ 398]                  blk.0.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[  10/ 398]                blk.0.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[  11/ 398]                blk.0.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  12/ 398]                blk.0.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  13/ 398]                  blk.0.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  14/ 398]                  blk.1.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  15/ 398]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  16/ 398]               blk.1.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  17/ 398]             blk.1.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  18/ 398]                  blk.1.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  19/ 398]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  20/ 398]                  blk.1.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[  21/ 398]                blk.1.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[  22/ 398]                blk.1.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  23/ 398]                blk.1.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  24/ 398]                  blk.1.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  25/ 398]                  blk.2.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  26/ 398]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  27/ 398]               blk.2.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  28/ 398]             blk.2.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  29/ 398]                  blk.2.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  30/ 398]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  31/ 398]                  blk.2.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[  32/ 398]                blk.2.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[  33/ 398]                blk.2.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  34/ 398]                blk.2.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  35/ 398]                  blk.2.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  36/ 398]                  blk.3.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  37/ 398]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  38/ 398]               blk.3.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  39/ 398]             blk.3.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  40/ 398]                  blk.3.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  41/ 398]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  42/ 398]                  blk.3.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[  43/ 398]                blk.3.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[  44/ 398]                blk.3.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  45/ 398]                blk.3.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  46/ 398]                  blk.3.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  47/ 398]                  blk.4.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  48/ 398]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  49/ 398]               blk.4.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  50/ 398]             blk.4.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  51/ 398]                  blk.4.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  52/ 398]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  53/ 398]                  blk.4.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  54/ 398]                blk.4.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  55/ 398]                blk.4.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  56/ 398]                blk.4.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  57/ 398]                  blk.4.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  58/ 398]                  blk.5.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  59/ 398]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  60/ 398]               blk.5.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  61/ 398]             blk.5.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  62/ 398]                  blk.5.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  63/ 398]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  64/ 398]                  blk.5.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  65/ 398]                blk.5.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  66/ 398]                blk.5.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  67/ 398]                blk.5.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  68/ 398]                  blk.5.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  69/ 398]                  blk.6.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  70/ 398]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  71/ 398]               blk.6.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  72/ 398]             blk.6.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  73/ 398]                  blk.6.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  74/ 398]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  75/ 398]                  blk.6.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[  76/ 398]                blk.6.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[  77/ 398]                blk.6.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  78/ 398]                blk.6.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  79/ 398]                  blk.6.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  80/ 398]                  blk.7.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  81/ 398]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  82/ 398]               blk.7.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  83/ 398]             blk.7.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  84/ 398]                  blk.7.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  85/ 398]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  86/ 398]                  blk.7.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  87/ 398]                blk.7.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  88/ 398]                blk.7.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  89/ 398]                blk.7.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  90/ 398]                  blk.7.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  91/ 398]                  blk.8.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  92/ 398]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  93/ 398]               blk.8.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[  94/ 398]             blk.8.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  95/ 398]                  blk.8.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[  96/ 398]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[  97/ 398]                  blk.8.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[  98/ 398]                blk.8.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[  99/ 398]                blk.8.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 100/ 398]                blk.8.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 101/ 398]                  blk.8.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 102/ 398]                  blk.9.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 103/ 398]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 104/ 398]               blk.9.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 105/ 398]             blk.9.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 106/ 398]                  blk.9.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 107/ 398]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 108/ 398]                  blk.9.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 109/ 398]                blk.9.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 110/ 398]                blk.9.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 111/ 398]                blk.9.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 112/ 398]                  blk.9.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 113/ 398]                 blk.10.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 114/ 398]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 115/ 398]              blk.10.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 116/ 398]            blk.10.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 117/ 398]                 blk.10.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 118/ 398]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 119/ 398]                 blk.10.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 120/ 398]               blk.10.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 121/ 398]               blk.10.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 122/ 398]               blk.10.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 123/ 398]                 blk.10.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 124/ 398]                 blk.11.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 125/ 398]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 126/ 398]              blk.11.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 127/ 398]            blk.11.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 128/ 398]                 blk.11.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 129/ 398]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 130/ 398]                 blk.11.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 131/ 398]               blk.11.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 132/ 398]               blk.11.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 133/ 398]               blk.11.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 134/ 398]                 blk.11.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 135/ 398]                 blk.12.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 136/ 398]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 137/ 398]              blk.12.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 138/ 398]            blk.12.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 139/ 398]                 blk.12.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 140/ 398]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 141/ 398]                 blk.12.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 142/ 398]               blk.12.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 143/ 398]               blk.12.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 144/ 398]               blk.12.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 145/ 398]                 blk.12.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 146/ 398]                 blk.13.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 147/ 398]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 148/ 398]              blk.13.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 149/ 398]            blk.13.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 150/ 398]                 blk.13.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 151/ 398]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 152/ 398]                 blk.13.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 153/ 398]               blk.13.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 154/ 398]               blk.13.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 155/ 398]               blk.13.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 156/ 398]                 blk.13.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 157/ 398]                 blk.14.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 158/ 398]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 159/ 398]              blk.14.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 160/ 398]            blk.14.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 161/ 398]                 blk.14.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 162/ 398]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 163/ 398]                 blk.14.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 164/ 398]               blk.14.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 165/ 398]               blk.14.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 166/ 398]               blk.14.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 167/ 398]                 blk.14.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 168/ 398]                 blk.15.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 169/ 398]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 170/ 398]              blk.15.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 171/ 398]            blk.15.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 172/ 398]                 blk.15.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 173/ 398]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 174/ 398]                 blk.15.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 175/ 398]               blk.15.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 176/ 398]               blk.15.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 177/ 398]               blk.15.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 178/ 398]                 blk.15.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 179/ 398]                 blk.16.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 180/ 398]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 181/ 398]              blk.16.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 182/ 398]            blk.16.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 183/ 398]                 blk.16.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 184/ 398]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 185/ 398]                 blk.16.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 186/ 398]               blk.16.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 187/ 398]               blk.16.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 188/ 398]               blk.16.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 189/ 398]                 blk.16.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 190/ 398]                 blk.17.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 191/ 398]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 192/ 398]              blk.17.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 193/ 398]            blk.17.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 194/ 398]                 blk.17.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 195/ 398]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 196/ 398]                 blk.17.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 197/ 398]               blk.17.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 198/ 398]               blk.17.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 199/ 398]               blk.17.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 200/ 398]                 blk.17.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 201/ 398]                 blk.18.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 202/ 398]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 203/ 398]              blk.18.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 204/ 398]            blk.18.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 205/ 398]                 blk.18.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 206/ 398]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 207/ 398]                 blk.18.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 208/ 398]               blk.18.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 209/ 398]               blk.18.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 210/ 398]               blk.18.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 211/ 398]                 blk.18.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 212/ 398]                 blk.19.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 213/ 398]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 214/ 398]              blk.19.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 215/ 398]            blk.19.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 216/ 398]                 blk.19.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 217/ 398]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 218/ 398]                 blk.19.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 219/ 398]               blk.19.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 220/ 398]               blk.19.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 221/ 398]               blk.19.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 222/ 398]                 blk.19.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 223/ 398]                 blk.20.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 224/ 398]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 225/ 398]              blk.20.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 226/ 398]            blk.20.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 227/ 398]                 blk.20.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 228/ 398]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 229/ 398]                 blk.20.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 230/ 398]               blk.20.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 231/ 398]               blk.20.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 232/ 398]               blk.20.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 233/ 398]                 blk.20.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 234/ 398]                 blk.21.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 235/ 398]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 236/ 398]              blk.21.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 237/ 398]            blk.21.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 238/ 398]                 blk.21.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 239/ 398]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 240/ 398]                 blk.21.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 241/ 398]               blk.21.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 242/ 398]               blk.21.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 243/ 398]               blk.21.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 244/ 398]                 blk.21.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 245/ 398]                 blk.22.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 246/ 398]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 247/ 398]              blk.22.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 248/ 398]            blk.22.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 249/ 398]                 blk.22.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 250/ 398]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 251/ 398]                 blk.22.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 252/ 398]               blk.22.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 253/ 398]               blk.22.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 254/ 398]               blk.22.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 255/ 398]                 blk.22.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 256/ 398]                 blk.23.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 257/ 398]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 258/ 398]              blk.23.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 259/ 398]            blk.23.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 260/ 398]                 blk.23.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 261/ 398]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 262/ 398]                 blk.23.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 263/ 398]               blk.23.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 264/ 398]               blk.23.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 265/ 398]               blk.23.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 266/ 398]                 blk.23.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 267/ 398]                 blk.24.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 268/ 398]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 269/ 398]              blk.24.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 270/ 398]            blk.24.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 271/ 398]                 blk.24.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 272/ 398]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 273/ 398]                 blk.24.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 274/ 398]               blk.24.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 275/ 398]               blk.24.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 276/ 398]               blk.24.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 277/ 398]                 blk.24.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 278/ 398]                 blk.25.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 279/ 398]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 280/ 398]              blk.25.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 281/ 398]            blk.25.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 282/ 398]                 blk.25.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 283/ 398]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 284/ 398]                 blk.25.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 285/ 398]               blk.25.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 286/ 398]               blk.25.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 287/ 398]               blk.25.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 288/ 398]                 blk.25.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 289/ 398]                 blk.26.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 290/ 398]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 291/ 398]              blk.26.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 292/ 398]            blk.26.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 293/ 398]                 blk.26.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 294/ 398]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 295/ 398]                 blk.26.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 296/ 398]               blk.26.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 297/ 398]               blk.26.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 298/ 398]               blk.26.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 299/ 398]                 blk.26.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 300/ 398]                 blk.27.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 301/ 398]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 302/ 398]              blk.27.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 303/ 398]            blk.27.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 304/ 398]                 blk.27.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 305/ 398]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 306/ 398]                 blk.27.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 307/ 398]               blk.27.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 308/ 398]               blk.27.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 309/ 398]               blk.27.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 310/ 398]                 blk.27.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 311/ 398]                 blk.28.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 312/ 398]            blk.28.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 313/ 398]              blk.28.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 314/ 398]            blk.28.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 315/ 398]                 blk.28.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 316/ 398]            blk.28.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 317/ 398]                 blk.28.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 318/ 398]               blk.28.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 319/ 398]               blk.28.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 320/ 398]               blk.28.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 321/ 398]                 blk.28.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 322/ 398]                 blk.29.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 323/ 398]            blk.29.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 324/ 398]              blk.29.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 325/ 398]            blk.29.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 326/ 398]                 blk.29.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 327/ 398]            blk.29.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 328/ 398]                 blk.29.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 329/ 398]               blk.29.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 330/ 398]               blk.29.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 331/ 398]               blk.29.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 332/ 398]                 blk.29.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 333/ 398]                 blk.30.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 334/ 398]            blk.30.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 335/ 398]              blk.30.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 336/ 398]            blk.30.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 337/ 398]                 blk.30.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 338/ 398]            blk.30.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 339/ 398]                 blk.30.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 340/ 398]               blk.30.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 341/ 398]               blk.30.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 342/ 398]               blk.30.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 343/ 398]                 blk.30.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 344/ 398]                 blk.31.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 345/ 398]            blk.31.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 346/ 398]              blk.31.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 347/ 398]            blk.31.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 348/ 398]                 blk.31.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 349/ 398]            blk.31.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 350/ 398]                 blk.31.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 351/ 398]               blk.31.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 352/ 398]               blk.31.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 353/ 398]               blk.31.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 354/ 398]                 blk.31.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 355/ 398]                 blk.32.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 356/ 398]            blk.32.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 357/ 398]              blk.32.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 358/ 398]            blk.32.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 359/ 398]                 blk.32.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 360/ 398]            blk.32.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 361/ 398]                 blk.32.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 362/ 398]               blk.32.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 363/ 398]               blk.32.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 364/ 398]               blk.32.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 365/ 398]                 blk.32.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 366/ 398]                 blk.33.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 367/ 398]            blk.33.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 368/ 398]              blk.33.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 369/ 398]            blk.33.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 370/ 398]                 blk.33.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 371/ 398]            blk.33.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 372/ 398]                 blk.33.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 373/ 398]               blk.33.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 374/ 398]               blk.33.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 375/ 398]               blk.33.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 376/ 398]                 blk.33.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 377/ 398]                 blk.34.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 378/ 398]            blk.34.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 379/ 398]              blk.34.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 380/ 398]            blk.34.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 381/ 398]                 blk.34.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 382/ 398]            blk.34.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 383/ 398]                 blk.34.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 384/ 398]               blk.34.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 385/ 398]               blk.34.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 386/ 398]               blk.34.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 387/ 398]                 blk.34.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 388/ 398]                 blk.35.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n[ 389/ 398]            blk.35.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 390/ 398]              blk.35.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 391/ 398]            blk.35.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 392/ 398]                 blk.35.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n[ 393/ 398]            blk.35.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n[ 394/ 398]                 blk.35.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n[ 395/ 398]               blk.35.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n[ 396/ 398]               blk.35.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n[ 397/ 398]               blk.35.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MB\n[ 398/ 398]                 blk.35.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\nllama_model_quantize_impl: model size  =  7672.62 MB\nllama_model_quantize_impl: quant size  =  2375.91 MB\n\nmain: quantize time = 215449.10 ms\nmain:    total time = 215449.10 ms\nUnsloth: Conversion completed! Output location: /kaggle/working/AbdelilahFdg/darija-chatt/unsloth.Q4_K_M.gguf\nUnsloth: Uploading GGUF to Huggingface Hub...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f67f6d81a5c42d7bbd0f872187af572"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsloth.Q4_K_M.gguf:   0%|          | 0.00/2.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8be62fc9515c4dc1b78a9f80373655fd"}},"metadata":{}},{"name":"stdout","text":"Saved GGUF to https://huggingface.co/AbdelilahFdg/darija-chatt\n","output_type":"stream"}],"execution_count":35}]}